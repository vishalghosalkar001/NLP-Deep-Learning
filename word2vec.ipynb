{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"word2vec.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPrryoWkSQm2j4RagHg90BG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import string\n","import nltk\n","nltk.download('stopwords')"],"metadata":{"id":"gGKN5tOpn1dA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Variables :\n","\n","* V    Number of unique words in our corpus of text ( Vocabulary )\n","\n","* x    Input layer (One hot encoding of our input word ). \n","* N    Number of neurons in the hidden layer of neural network\n","* W    Weights between input layer and hidden layer\n","* W'   Weights between hidden layer and output layer\n","* y    A softmax output layer having probabilities of every word in our vocabulary"],"metadata":{"id":"VNoamWLmnWAE"}},{"cell_type":"code","source":[" \n","def softmax(x):\n","    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n","    e_x = np.exp(x - np.max(x))\n","    return e_x / e_x.sum()\n","  \n","class word2vec(object):\n","    def __init__(self):\n","        self.N = 10\n","        self.X_train = []\n","        self.y_train = []\n","        self.window_size = 2\n","        self.alpha = 0.001\n","        self.words = []\n","        self.word_index = {}\n","  \n","    def initialize(self,V,data):\n","        self.V = V\n","        self.W = np.random.uniform(-0.8, 0.8, (self.V, self.N))\n","        self.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V))\n","          \n","        self.words = data\n","        for i in range(len(data)):\n","            self.word_index[data[i]] = i\n","  \n","      \n","    def feed_forward(self,X):\n","        self.h = np.dot(self.W.T,X).reshape(self.N,1)\n","        self.u = np.dot(self.W1.T,self.h)\n","        #print(self.u)\n","        self.y = softmax(self.u) \n","        return self.y\n","          \n","    def backpropagate(self,x,t):\n","        e = self.y - np.asarray(t).reshape(self.V,1)\n","        # e.shape is V x 1\n","        dLdW1 = np.dot(self.h,e.T)\n","        X = np.array(x).reshape(self.V,1)\n","        dLdW = np.dot(X, np.dot(self.W1,e).T)\n","        self.W1 = self.W1 - self.alpha*dLdW1\n","        self.W = self.W - self.alpha*dLdW\n","          \n","    def train(self,epochs):\n","        for x in range(1,epochs):       \n","            self.loss = 0\n","            for j in range(len(self.X_train)):\n","                self.feed_forward(self.X_train[j])\n","                self.backpropagate(self.X_train[j],self.y_train[j])\n","                C = 0\n","                for m in range(self.V):\n","                    if(self.y_train[j][m]):\n","                        self.loss += -1*self.u[m][0]\n","                        C += 1\n","                self.loss += C*np.log(np.sum(np.exp(self.u)))\n","            print(\"epoch \",x, \" loss = \",self.loss)\n","            self.alpha *= 1/( (1+self.alpha*x) )\n","             \n","    def predict(self,word,number_of_predictions):\n","        if word in self.words:\n","            index = self.word_index[word]\n","            X = [0 for i in range(self.V)]\n","            X[index] = 1\n","            prediction = self.feed_forward(X)\n","            output = {}\n","            for i in range(self.V):\n","                output[prediction[i][0]] = i\n","              \n","            top_context_words = []\n","            for k in sorted(output,reverse=True):\n","                top_context_words.append(self.words[output[k]])\n","                if(len(top_context_words)>=number_of_predictions):\n","                    break\n","      \n","            return top_context_words\n","        else:\n","            print(\"Word not found in dictionary\")"],"metadata":{"id":"uu2XgYfoeA2l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocessing(corpus):\n","    stop_words = set(stopwords.words('english'))   \n","    training_data = []\n","    sentences = corpus.split(\".\")\n","    for i in range(len(sentences)):\n","        sentences[i] = sentences[i].strip()\n","        sentence = sentences[i].split()\n","        x = [word.strip(string.punctuation) for word in sentence\n","                                     if word not in stop_words]\n","        x = [word.lower() for word in x]\n","        training_data.append(x)\n","    return training_data\n","      \n","  \n","def prepare_data_for_training(sentences,w2v):\n","    data = {}\n","    for sentence in sentences:\n","        for word in sentence:\n","            if word not in data:\n","                data[word] = 1\n","            else:\n","                data[word] += 1\n","    V = len(data)\n","    data = sorted(list(data.keys()))\n","    vocab = {}\n","    for i in range(len(data)):\n","        vocab[data[i]] = i\n","      \n","    #for i in range(len(words)):\n","    for sentence in sentences:\n","        for i in range(len(sentence)):\n","            center_word = [0 for x in range(V)]\n","            center_word[vocab[sentence[i]]] = 1\n","            context = [0 for x in range(V)]\n","             \n","            for j in range(i-w2v.window_size,i+w2v.window_size):\n","                if i!=j and j>=0 and j<len(sentence):\n","                    context[vocab[sentence[j]]] += 1\n","            w2v.X_train.append(center_word)\n","            w2v.y_train.append(context)\n","    w2v.initialize(V,data)\n","  \n","    return w2v.X_train,w2v.y_train  "],"metadata":{"id":"DPsj4fhFeKvq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus = \"\"\n","corpus += \"The future king is the prince. Daughter is the princess . Son is the prince. Only a man can be a king . Only a woman can be a queen. The princess will be a queen. Queen and king rule the realm. The prince is a strong man. The princess is a beautiful woman . Prince is only a boy now. A boy will be a man. The royal family is the king and queen and their children\"\n","epochs = 1000\n"," \n","training_data = preprocessing(corpus)\n","w2v = word2vec()\n"," \n","prepare_data_for_training(training_data,w2v)\n","w2v.train(epochs)\n"],"metadata":{"id":"tgod8Qbue9DI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(w2v.predict(\"king\",3))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uYVJje_nkzaD","executionInfo":{"status":"ok","timestamp":1648589563764,"user_tz":240,"elapsed":137,"user":{"displayName":"ki lee","userId":"02764728694704026136"}},"outputId":"2ed96474-d2d7-4643-9dee-538a9a4f53dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['queen', 'man', 'family']\n"]}]},{"cell_type":"markdown","source":["#CBOW (Continuous Bag of Words) : \n","\n","CBOW model predicts the current word given context words within specific window. The input layer contains the context words and the output layer contains the current word. The hidden layer contains the number of dimensions in which we want to represent current word present at the output layer."],"metadata":{"id":"zyuQqFNGmv6l"}},{"cell_type":"code","source":["# Python program to generate word vectors using Word2Vec\n","  \n","# importing all necessary modules\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","import warnings\n","  \n","warnings.filterwarnings(action = 'ignore')\n","  \n","import gensim\n","from gensim.models import Word2Vec\n","  \n","#  Reads â€˜Alice.txtâ€™ file\n","sample = open(\"/content/sample_data/Alice.txt\", \"r\")\n","s = sample.read()\n","  \n","# Replaces escape character with space\n","f = s.replace(\"\\n\", \" \")\n","  \n","data = []\n","  \n","# iterate through each sentence in the file\n","for i in sent_tokenize(f):\n","    temp = []\n","      \n","    # tokenize the sentence into words\n","    for j in word_tokenize(i):\n","        temp.append(j.lower())\n","  \n","    data.append(temp)\n","  \n","# Create CBOW model\n","model1 = gensim.models.Word2Vec(data, min_count = 1, \n","                              size = 100, window = 5)\n","  \n","# Print results\n","print(\"Cosine similarity between 'alice' \" + \n","               \"and 'wonderland' - CBOW : \",\n","    model1.similarity('alice', 'wonderland'))\n","      \n","print(\"Cosine similarity between 'alice' \" +\n","                 \"and 'machines' - CBOW : \",\n","      model1.similarity('alice', 'machines'))\n","  "],"metadata":{"id":"55fkoHbhlaIu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# Skip Gram : \n","\n","Skip gram predicts the surrounding context words within specific window given current word. The input layer contains the current word and the output layer contains the context words. The hidden layer contains the number of dimensions in which we want to represent current word present at the input layer."],"metadata":{"id":"tKtO-PC8nBaC"}},{"cell_type":"code","source":["\n","# Create Skip Gram model\n","model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100,\n","                                             window = 5, sg = 1)\n","  \n","# Print results\n","print(\"Cosine similarity between 'alice' \" +\n","          \"and 'wonderland' - Skip Gram : \",\n","    model2.similarity('alice', 'wonderland'))\n","      \n","print(\"Cosine similarity between 'alice' \" +\n","            \"and 'machines' - Skip Gram : \",\n","      model2.similarity('alice', 'machines'))"],"metadata":{"id":"NAjhHnf2lt_X"},"execution_count":null,"outputs":[]}]}